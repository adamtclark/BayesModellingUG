---
title: "Bayesian Stats, Meeting 3"
author: "Adam Clark"
date: "2023-07-11"
output: html_document
---
  
## More complex case: random slopes and intercepts

Okay, now let's try a somewhat more complex case, where both the slope and the intercepts vary across blocks. Let's keep a few observations (2 plots) per block this time.

```{r echo=TRUE}
require(nlme) # load package

# Multiple regression example
set.seed(231011) # seed for analyses
error = 10 # residual error
n_blocks = 10 # number of blocks
intercepts = sort(rnorm(n_blocks, mean = 200, sd = 80)) # y-intercepts for each block
slopes = rnorm(n_blocks, mean = 5, sd = 4) # slope


# make fake data for 50 plots
bdat = data.frame(plot = 1:50,
                  block = rep(1:n_blocks, each = 50),
                  diversity = sample(1:32, 50*n_blocks, rep = TRUE), # randomly select richness between 1 and 32
                  biomass = NA)

# generate random biomass data
bdat$biomass=intercepts[bdat$block] + slopes[bdat$block]*bdat$diversity + rnorm(nrow(bdat), 0, error)

# keep just 2 plots for the analysis
bdat_slopes = bdat[bdat$plot %in% c(1,2),]

# fit a mixed-effects regression
mod_mixef2 = lme(biomass~diversity, data = bdat_slopes, random = ~1+diversity|block)

# look at summary output - what do these terms tell us?
summary(mod_mixef2)

# look at random effects
ranef(mod_mixef2)

# plot full dataset
par(mar=c(4,4,2,2))
plot(biomass~diversity, data = bdat, col = rainbow(n_blocks)[bdat$block])

# add lines
for(i in 1:n_blocks) {
  curve(fixef(mod_mixef2)[1]+ranef(mod_mixef2)[i,1] + (fixef(mod_mixef2)[2]+ranef(mod_mixef2)[i,2])*x, from = 1, to = 32, add = TRUE, lty = 2, col = rainbow(n_blocks)[i])
}

# plot observed vs. predicted
prd = predict(mod_mixef2, newdata = bdat)
plot(prd, bdat$biomass,
     xlab = "prediction", ylab = "observation", col = rainbow(n_blocks)[bdat$block])
abline(a=0, b = 1, lty = 2)

# root mean square error:
sqrt(mean((prd-bdat$biomass)^2))
```

Not too bad! In contrast, look what happens when we try to fit a regular multiple regression to the same data:
  
```{r echo=TRUE, warning=FALSE}
# fit a mixed-effects regression
mod_mult = lm(biomass~diversity*factor(block), data = bdat_slopes)

# look at summary output - what do these terms tell us?
summary(mod_mult)

# plot
par(mar=c(4,4,2,2))
plot(biomass~diversity, data = bdat_slopes, col = rainbow(n_blocks)[bdat_slopes$block])

# add lines
dsq = 1:32
for(i in 1:n_blocks) {
  pred = predict(mod_mult, newdata = data.frame(diversity = dsq, block = i))
  lines(dsq, pred, col = rainbow(n_blocks)[i], lty = 2)
}

# plot observed vs. predicted
prd_multi = predict(mod_mult, newdata = bdat)

plot(prd_multi, bdat$biomass,
     xlab = "prediction", ylab = "observation", col = rainbow(n_blocks)[bdat$block])
abline(a=0, b = 1, lty = 2)

# root mean square error:
sqrt(mean((prd_multi-bdat$biomass)^2))
```

Notice, we still get estimates of the slope and intercept, but no estimate of uncertainty (because of the small sample size). And, the fit is quite a bit worse. Again, we managed to get "better" estimates by "borrowing power" (grouping uncertainty) from across observations.

## Your turn: Dataset 1

Try analysing the following dataset using the `lme` function.

```{r echo=TRUE}
bdat = read.csv("dataset_1.csv")
n_blocks = length(unique(bdat$block))

# plot
par(mar=c(4,4,2,2))
plot(biomass~diversity, col = rainbow(n_blocks)[bdat$block], data = bdat)
```

What form of regression should you use? Random slopes? Random intercepts? Something else?
  
What do the results of the regression show us? What are the fixed and random effects estimates? What is the variability across parameters?
  

## Nested models
  
One of the most common uses of mixed effects models is to deal with "nested" data structures - e.g. cases where we have plots and subplots, and we want to control for potential autocorrelation in measurements across these different levels.

As an example, consider the following dataset:
  
```{r echo=TRUE}
# Nested data example
set.seed(231011) # seed for analyses
error = 10 # residual error
n_plots = 100 # number of plots
n_subplots = 10 # subplots per block
intercepts_plots = sort(rnorm(n_plots, mean = 200, sd = 40)) # y-intercepts for each plot

intercepts_subplots = matrix(nrow = n_plots, ncol = n_subplots)
for(i in 1:n_plots) {
  intercepts_subplots[i,] = rnorm(n_subplots, mean = intercepts_plots[i], sd = 20) # intercept for each subplot
}

head(intercepts_subplots) # look at coefficient matrix

# make fake data
bdat = data.frame(plot = rep(1:n_plots, each = n_subplots),
                  subplot = 1:n_subplots,
                  biomass = NA)

bdat$biomass = intercepts_subplots[cbind(bdat$plot, bdat$subplot)] + rnorm(n_plots*n_subplots, 0, error)
```

Now, let's first go ahead and fit *just* a model of plot-level effects. Before reading the code below, try writing out a version of this model yourself.

Note that we only are modelling mean biomass in each plot (i.e. the "intercepts") rather than a relation with diversity. This will result in a "random effects model" - i.e. a mixed effects model with no fixed effects (other than the y-intercept). This is actually the kind of analysis for which mixed effects models were initially developed to conduct (e.g. for systems with lots of categorical predictor variables, such as differences in manufacturing outputs across employees in a big company).

```{r echo=TRUE}
# Analyse data
mmod = lme(biomass ~ 1, data = bdat, random = ~1|plot)

summary(mmod)
```

To fit a nested model that also includes subplot-level effects, we simply add another argument to the random effect term:

```{r echo=TRUE}
# Analyse data
mmod2 = lme(biomass ~ 1, data = bdat, random = ~1|plot/subplot)

summary(mmod2)
```

Not, let's see how good a job we did of estimating the variability at each level in this model. Remember, residual error should be 30, plot-level variability should be 40, and subplot-level variability should be 20 (based on the standard deviations in the script above).

```{r echo=TRUE}
VarCorr(mmod2)

VarCorr(mmod2)[,2] # look at standard deviations
```

Note, we are close, but not exactly there (we've under-estimated subplot-level and residual-level variability). See notes below for a breif explanation of this point. Among other things, note that we have no estimate of uncertainty in these variability estimates (e.g. we can't test whether they are significantly different from zero). One way to address this is to apply model comparison, which we will do below. We could also use other methods to fit the model - e.g. bootstrapping, or MCMC. We'll apply these tools in later weeks.

## Model selection

Just like a regular OLS, we can compare mixed effects models using things like ANOVA or AIC. However, things are a bit different - in general, we need use a different fitting algorithm ("ML", or "maximum likelihood") when we are fitting models for the purpose of model comparison, and then return to the default fitting algorithm ("REML", or "reduced maximum likelihood") once we have identified the best model. Here's an example of how to do so:
  
```{r echo=TRUE}
# re-fit models
ML_mmod = update(mmod, method = "ML")
ML_mmod2 = update(mmod2, method = "ML")

# run ANOVA
anova(ML_mmod, ML_mmod2)
```

The p-value is less than 0.05 - this suggests that the more complex model - mmod2 - adds significant explanatory power - i.e. that there is structuring of residuals at the subplot level (which we already knew since we simulated the data that way). Note that if we try to do the same analysis, but with some random extra variable that we know has no influence of the data:
  
```{r echo=TRUE}
bdat$fake_subplot = sample(1:50, n_plots*n_subplots, replace = TRUE)

ML_mmod3 = lme(biomass ~ 1, data = bdat, random = ~1|plot/fake_subplot, method = "ML")

# run ANOVA
anova(ML_mmod, ML_mmod3)
```

we end up with p > 0.05 - i.e. the test tells us that there is no significant addition of information value to the model based on the additional fake_subplot data (which is encouraging...).

Remeber that once we've found our "best" model, we need to re-fit in order to interpret it:

```{r echo=TRUE}
mmod2 = update(ML_mmod2, method = "REML")
summary(mmod2)
```

You can apply these test to all kinds of models. ANOVA apply for any nested set of models (i.e. models that differ in only a single random effect or a single fixed effect), whereas AIC can be applied for (most) any analyses that share the same response variables. Remember that for AIC, smaller values mean better fitting models (though also note that AIC can be under-conservative for big datas´ets). We'll discuss more complex examples in later classes.´

## Your turn: Dataset 2:

Try fitting different structures of models (combinations of fixed effect slopes and intercepts, and random effects vs. nested random effects).

```{r echo=TRUE}
bdat = read.csv("dataset_2.csv")
n_plots = length(unique(bdat$plot))

# plot
par(mar=c(4,4,2,2))
plot(biomass~diversity, col = rainbow(n_plots)[plot], pch = subplot, data = bdat)
```

Note - different point types show different subplots, and different colors show different plots. We can plot these trends a bit more informatively using the `coplot` command. It is set up similarly to the `lme` function (with some extra bits that we'll discuss later).

```{r echo=TRUE}
par(mar=c(4,4,2,2))
coplot(biomass~diversity|as.factor(plot), data = bdat,
       panel = function(x, y, ...) {points(x,y, pch = bdat$subplot[bdat$plot==1])})
```

What form of regression should you use? Random slopes? Random intercepts? Nested? Something else?

What do the results of the regression show us? What are the fixed and random effects estimates? What is the variability across parameters?

## Next week:

Work through the datasets described here, and post any questions you have on the blog on the course website. In addition, try to find a "real-world" example of a dataset (e.g. from your research) that can be analysed using the tools we've discussed today, and try applying them. What model structure fits best? What does this structure hypothesise about the data? What do your results tell you?

We'll discuss these points together in the coming weeks, so please be sure to write up your analyses in well-commented code, and to send them out over the blog.

Next time we meet, we'll expand on the topics we've discussed this far using two additional packages: `lme4`, and `brms`.
